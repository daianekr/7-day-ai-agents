{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b49d9b4-5885-4983-b71f-d74255db3adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c09fa18-dd25-46d3-b381-21b2cc4be94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAQ documents: 1219\n",
      "Evidently documents: 95\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "evidently_docs = read_repo_data('evidentlyai', 'docs')\n",
    "\n",
    "print(f\"FAQ documents: {len(dtc_faq)}\")\n",
    "print(f\"Evidently documents: {len(evidently_docs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "956743fc-dfb5-4f08-a513-48b86a0712b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f77f80f-aa49-494d-b9c4-bf93f50f2686",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for chunk in chunks:\n",
    "        chunk.update(doc_copy)\n",
    "    evidently_chunks.extend(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d65f13-7027-4043-b373-b2380104ffbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1e92e429-51ce-4c7a-b00b-75facbcddcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "846b1cd1-3a28-41f9-993f-079414330d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70666f8e-5ccc-41d0-895f-c3d5f82233f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()\n",
    "\n",
    "\n",
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model='gpt-4o-mini',\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aad07a4b-575f-4f33-ba31-776c28e3cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Split the provided document into logical sections\n",
    "that make sense for a Q&A system.\n",
    "\n",
    "Each section should be self-contained and cover\n",
    "a specific topic or concept.\n",
    "\n",
    "<DOCUMENT>\n",
    "{document}\n",
    "</DOCUMENT>\n",
    "\n",
    "Use this format:\n",
    "\n",
    "## Section Name\n",
    "\n",
    "Section content with all relevant details\n",
    "\n",
    "---\n",
    "\n",
    "## Another Section Name\n",
    "\n",
    "Another section content\n",
    "\n",
    "---\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00a1c1e4-8cb6-4c46-be4b-523603fe78f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intelligent_chunking(text):\n",
    "    prompt = prompt_template.format(document=text)\n",
    "    response = llm(prompt)\n",
    "    sections = response.split('---')\n",
    "    sections = [s.strip() for s in sections if s.strip()]\n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46be831b-9a63-4859-81c8-5e24ee29c215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da0b78640a841bd86852e5671de8cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "evidently_chunks = []\n",
    "\n",
    "for doc in tqdm(evidently_docs):\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "\n",
    "    sections = intelligent_chunking(doc_content)\n",
    "    for section in sections:\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc['section'] = section\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ea6def-8dd2-4a82-8144-20a242249062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x11e113250>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d7b8ac0-3af6-4ee5-b165-8d8ddf35d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc9a503-88b0-47b7-8cbc-a8b57a38d4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx', 'section': '## Creating a RAG Test Dataset\\n\\nYou can generate a ground truth RAG dataset from your data source.\\n\\n### Steps to Create a RAG Test Dataset\\n\\n1. **Create a Project**\\n   - In the Evidently UI, start a new Project or open an existing one.\\n   - Navigate to “Datasets” in the left menu.\\n   - Click “Generate” and select the “RAG” option.\\n   ![](/images/synthetic/synthetic_data_select_method.png)\\n\\n2. **Upload Your Knowledge Base**\\n   - Select a file containing the information your AI system retrieves from. Supported formats: Markdown (.md), CSV, TXT, PDFs.\\n   - Choose how many inputs to generate.\\n   ![](/images/synthetic/synthetic_data_inputs_example_upload.png)\\n   - Simply drop the file, then:\\n     - Choose the number of inputs to generate.\\n     - Decide if you want to include the context used to generate the answer.\\n   ![](/images/synthetic/synthetic_data_inputs_example_upload2.png)\\n   - The system automatically extracts relevant facts and generates user-like questions to your data source with ground truth answers.\\n   <Info>\\n     Note that it may take some time to process the dataset. Limits apply on the free plan.\\n   </Info>\\n\\n3. **Review the Test Cases**\\n   - You can preview and refine the generated dataset.\\n   ![](/images/synthetic/synthetic_data_rag_example_result.png)\\n   - You can:\\n     - Use “More like this” to add more variations.\\n     - Drop rows that aren’t relevant.\\n     - Manually edit questions or responses.\\n\\n4. **Save the Dataset**\\n   - Once you are finished, store the dataset. \\n   - You can download it as a CSV file or access it via the Python API using the dataset ID to use in your evaluation.\\n   <Info>\\n     **Dataset API.** How to work with [Evidently datasets](/docs/platform/datasets_overview).\\n   </Info>'}, {'title': 'RAG evaluation dataset', 'description': 'Synthetic data for RAG.', 'filename': 'docs-main/synthetic-data/rag_data.mdx', 'section': '## Retrieval-Augmented Generation (RAG) Overview\\n\\nRetrieval-Augmented Generation (RAG) systems rely on retrieving answers from a knowledge base before generating responses. To evaluate them effectively, you need a test dataset that reflects what the system *should* know.'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 3. Run Evaluations\\n\\nEvaluate the answers based on sentiment, text length, and denials.\\n\\nSet the OpenAI API key as an environment variable:\\n\\n```python\\n## import os\\n## os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\nRun evaluations:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    ...\\n)\\n```\\n\\nPreview results locally in pandas:\\n\\n```python\\neval_dataset.as_dataframe()\\n```'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 5. Get a Dashboard\\n\\nCreate a Dashboard in Evidently Cloud to track evaluations over time.'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 6. (Optional) Add Tests\\n\\nImplement conditions for evaluations, such as ensuring sentiment is non-negative or denying refusals:\\n\\n```python\\neval_dataset = Dataset.from_pandas(\\n    eval_df,\\n    ...\\n)\\n```\\n\\nView summary reports based on tests.'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 2. Prepare the Dataset\\n\\nCreate a demo chatbot dataset with \"Questions\" and \"Answers\".\\n\\n```python\\ndata = [\\n    [\"What is the chemical symbol for gold?\", \"Gold chemical symbol is Au.\"],\\n    ...\\n]\\ncolumns = [\"question\", \"answer\"]\\n\\neval_df = pd.DataFrame(data, columns=columns)\\n```\\n\\n### Preparing Your Own Data\\n\\nYou can input data with various structures, including LLM input-output pairs or reference comparisons.\\n\\n### Collecting Live Data\\n\\nTrace inputs and outputs from your LLM application and download the dataset.'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 7. (Optional) Add a Custom LLM Judge\\n\\nUtilize built-in LLM judge templates to create custom criteria for evaluations:\\n\\n```python\\nappropriate_scope = BinaryClassificationPromptTemplate(\\n    ...\\n)\\n\\nllm_evals = Dataset.from_pandas(\\n    eval_df,\\n    ...\\n)\\n\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(llm_evals, None)\\n```'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': \"## What's Next?\\n\\nExplore further configurations for LLM judges or review additional tutorials available in the Evidently documentation.\"}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## 1. Set Up Your Environment\\n\\n### 1.1. Set Up Evidently Cloud\\n\\n<CloudSignup />\\n\\n### 1.2. Installation and Imports\\n\\nInstall the Evidently Python library:\\n\\n```python\\n!pip install evidently\\n```\\n\\nImport necessary components to run evaluations.\\n\\n### 1.3. Create a Project\\n\\n<CreateProject />'}, {'title': 'LLM Evaluation', 'description': 'Evaluate text outputs in under 5 minutes', 'filename': 'docs-main/quickstart_llm.mdx', 'section': '## Quickstart Guide\\n\\nThis Quickstart will help you run a simple evaluation in Python and view the results in Evidently Cloud. If you prefer a fully local setup, you can skip certain steps.'}]\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f668ac-9f69-403f-9b5d-fe1bc050533e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x106b32c90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f23cc584-ba83-4395-b0d9-44c7046116e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x106b32c90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "faq_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc6ecd23-106d-44c3-bd7b-d34823debda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad54d8ca37ea41469d4cac0b6fedf385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ec9824324840dfb8e4bcad0b1e4c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "934de8c8dd5647c1997e197add16e236",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caca89dbf11e43f6ab38f83efbe3d1fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb632c5e2fc943c2b693a86c6891f132",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/523 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30eb32caf1a14c9786dc47cd599c37c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ff6c9ade1914653a98958dc75239fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b42eb030eec422fbd1faee84b543822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8b48180453400eb8f2f1cf8f678eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00186ce6066548a69ab32161e1ca65ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8aa06c7f1c48e7a035553d9f53159c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d35aac2e-4434-4449-bc3a-639615fa3aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16998c8b-0dbf-4f41-9c12-2acc5143ef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9e52e37-1771-4409-8a76-44b53855c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = v_query.dot(v_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "04461031-d943-48ef-9a8c-9629815fdbf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f8b38c945945bf9e9e30f641063298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "805edd8b-a44d-45ec-ba8c-5e4e5b87bae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x167e7e2d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45f13a4f-ef44-4099-a971-45df7414c2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b7fcdb8-831e-4c1e-a099-0e77c4bc2a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'},\n",
       " {'id': '900f60fd25',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'sort_order': 15,\n",
       "  'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'},\n",
       " {'id': '721f9e0c29',\n",
       "  'question': 'How can we contribute to the course?',\n",
       "  'sort_order': 35,\n",
       "  'content': '- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\\n- Share it with friends if you find it useful.\\n- Create a pull request (PR) if you can improve the text or structure of the repository.\\n- [Update this FAQ](https://github.com/DataTalksClub/faq/).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md'},\n",
       " {'id': '6314bc3029',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_1813f02b.png'}],\n",
       "  'question': 'How do I get my certificate?',\n",
       "  'sort_order': 46,\n",
       "  'content': 'There\\'ll be an announcement in Telegram and the course channel for:\\n\\n- Checking that your full name is displayed correctly on the Certificate (see Editing course profile on the Course Management webpage).\\n- Notifying when the grading is completed.\\n\\nYou will find it in your course profile (you need to be\\nlogged it). \\n\\nFor 2025 the link to the course profile is this:\\n\\n`https://courses.datatalks.club/de-zoomcamp-2025/enrollment`\\n\\nFor other editions, change \"2025\" to your edition.\\n\\nAfter the second announcement, follow instructions in [certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md) on how to generate the Certificate document yourself.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md'},\n",
       " {'id': '16005581f2',\n",
       "  'question': 'Edit Course Profile.',\n",
       "  'sort_order': 13,\n",
       "  'content': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname or your real name if you prefer. Your entry on the Leaderboard is the one highlighted in light green.\\n\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\n\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/013_16005581f2_edit-course-profile.md'},\n",
       " {'id': 'b7542b8d36',\n",
       "  'question': 'Environment: Is the course [Windows/macOS/Linux/...] friendly?',\n",
       "  'sort_order': 36,\n",
       "  'content': 'Yes! Linux is ideal but technically it should not matter. Students in the 2024 cohort used all 3 OSes successfully.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/036_b7542b8d36_environment-is-the-course-windowsmacoslinux-friend.md'},\n",
       " {'id': 'dc06a38bc6',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'sort_order': 42,\n",
       "  'content': 'After you create a GitHub account, clone the course repo to your local machine using the process outlined in this video:\\n\\n[Git for Everybody: How to Clone a Repository from GitHub](https://www.youtube.com/watch?v=CKcqniGu3tA).\\n\\nHaving this local repository on your computer will make it easy to access the instructors’ code and make pull requests if you want to add your own notes or make changes to the course content.\\n\\nYou will probably also create your own repositories to host your notes and versions of files. Here is a great tutorial that shows you how to do this:\\n\\n[How to Create a Git Repository](https://www.atlassian.com/git/tutorials/setting-up-a-repository).\\n\\nRemember to ignore large databases, .csv, and .gz files, and other files that should not be saved to a repository. Use `.gitignore` for this:\\n\\n[.gitignore file](https://www.atlassian.com/git/tutorials/saving-changes/gitignore).\\n\\n**Important:**\\n\\n**NEVER store passwords or keys in a git repo** (even if the repo is set to private). Put files containing sensitive information (.env, secret.json, etc.) in your `.gitignore`.\\n\\nThis is also a great resource: [Dangit, Git!?!](https://dangitgit.com/)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/042_dc06a38bc6_how-do-i-use-git-github-for-this-course.md'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "168914fd-f8a7-4fc0-add9-8a80e9bf7b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "26086148-78ba-46ca-9d41-9691349e1641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': '33fc260cd8',\n",
       "  'question': 'Course: What can I do before the course starts?',\n",
       "  'sort_order': 5,\n",
       "  'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'},\n",
       " {'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'},\n",
       " {'id': '900f60fd25',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'sort_order': 15,\n",
       "  'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83f54f01-b2ef-4cdd-99b5-9aa0032b16df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a213cc3d-9883-45d5-9126-997eb78a9baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if you can join the course now, it’s best to check the course details or the official website for enrollment deadlines. If the course is still open for registration, you should be able to sign up. If not, you might want to contact the course administrators for any possible late enrollment options.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09d832c0-757a-43b3-9a29-92018c72efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8bdf0b4-fe9d-48ec-90fc-95ede554f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67d29953-9b9e-4ace-853d-9e151d3460e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\"\"\"\n",
    "\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58bbc690-368f-421b-af39-e5206322f59b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_68daff40e6788195a301f9a2ca3d250f0b37f1ce7414cab0', created_at=1759182657.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"query\":\"can I join the course late\"}', call_id='call_oDTr9KYeah4jmWcYFGdrEF7Q', name='text_search', type='function_call', id='fc_68daff41e63c8195a5c81cdfb118677b0b37f1ce7414cab0', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='text_search', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query text to look up in the course FAQ.'}}, 'required': ['query'], 'additionalProperties': False}, strict=True, type='function', description='Search the FAQ database')], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=74, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=94), user=None, billing={'payer': 'developer'}, store=True)\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a609053-738a-40ec-864a-f02079e31d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "call = response.output[0]\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3fc758bd-dea5-4751-8bb5-cbf155887095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function_call_output',\n",
       " 'call_id': 'call_oDTr9KYeah4jmWcYFGdrEF7Q',\n",
       " 'output': '[{\"id\": \"3f1424af17\", \"question\": \"Course: Can I still join the course after the start date?\", \"sort_order\": 3, \"content\": \"Yes, even if you don\\'t register, you\\'re still eligible to submit the homework.\\\\n\\\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don\\'t leave everything for the last minute.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md\"}, {\"id\": \"9e508f2212\", \"question\": \"Course: When does the course start?\", \"sort_order\": 1, \"content\": \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\\\n\\\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\\\n- Don\\\\u2019t forget to register in DataTalks.Club\\'s Slack and join the channel.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md\"}, {\"id\": \"068529125b\", \"question\": \"Course - Can I follow the course after it finishes?\", \"sort_order\": 8, \"content\": \"Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\\\n\\\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md\"}, {\"id\": \"4dbd2eea47\", \"question\": \"Homework: Are late submissions of homework allowed?\", \"sort_order\": 17, \"content\": \"No, late submissions are not allowed. However, if the form is still open after the due date, you can still submit the homework. Confirm your submission by checking the date-timestamp on the Course page. Ensure you are logged in.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/017_4dbd2eea47_homework-are-late-submissions-of-homework-allowed.md\"}, {\"id\": \"3774a79c13\", \"question\": \"Certificate: Do I need to do the homeworks to get the certificate?\", \"sort_order\": 14, \"content\": \"No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example.\", \"filename\": \"faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md\"}]'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "696719af-b9fb-4601-b6cb-795a451de001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the course even if it has already started. While you don't need to register to submit homework, keep in mind that there are deadlines for turning in assignments and final projects. \n",
      "\n",
      "If you want to participate, just be aware of those deadlines and try not to leave everything until the last minute!\n"
     ]
    }
   ],
   "source": [
    "chat_messages.append(call)\n",
    "chat_messages.append(call_output)\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99c79c3d-242d-4eee-a9b0-15736f5a966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4bf37b8-1ca0-4619-9f81-a7692877dca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3faacd49-b21d-4fa0-a5a0-aa969c928c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6af9ad85-e4a6-41fa-9aed-6654ed822701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=\"Yes, you can still join the course even after the start date. Here are some details:\\n\\n- **Homework Submission**: You are eligible to submit homework even if you don't register.\\n- **Deadlines**: Be aware that there are deadlines for submitting homework and final projects, so try not to leave everything until the last minute.\\n\\nAdditionally, after the course finishes, all materials will still be available for you to follow at your own pace. If you're interested, you can register before the next cohort starts on **January 13th, 2025** using the provided [registration link](https://airtable.com/shr6oVXeQvSI5HuWD). \\n\\nFeel free to ask if you have more questions!\")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9911908-2573-4dcb-bfbc-348cec138f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='I just discovered the course, can I join now?', timestamp=datetime.datetime(2025, 9, 29, 21, 58, 58, 924733, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant for a course.'),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"join course\"}', tool_call_id='call_dblQdeL5538IjgmPzKkLpj0a')], usage=RequestUsage(input_tokens=111, output_tokens=15, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 29, 21, 59, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'tool_calls'}, provider_response_id='chatcmpl-CLGBYMipDg9AmrBS0E8WiBmVcsiLp', finish_reason='tool_call'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': 'bfafa427b3', 'question': 'Course: What are the prerequisites for this course?', 'sort_order': 2, 'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'}, {'id': '16005581f2', 'question': 'Edit Course Profile.', 'sort_order': 13, 'content': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname or your real name if you prefer. Your entry on the Leaderboard is the one highlighted in light green.\\n\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\n\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/013_16005581f2_edit-course-profile.md'}], tool_call_id='call_dblQdeL5538IjgmPzKkLpj0a', timestamp=datetime.datetime(2025, 9, 29, 21, 59, 1, 9782, tzinfo=datetime.timezone.utc))], instructions='You are a helpful assistant for a course.'),\n",
       " ModelResponse(parts=[TextPart(content=\"Yes, you can still join the course even after the start date. Here are some details:\\n\\n- **Homework Submission**: You are eligible to submit homework even if you don't register.\\n- **Deadlines**: Be aware that there are deadlines for submitting homework and final projects, so try not to leave everything until the last minute.\\n\\nAdditionally, after the course finishes, all materials will still be available for you to follow at your own pace. If you're interested, you can register before the next cohort starts on **January 13th, 2025** using the provided [registration link](https://airtable.com/shr6oVXeQvSI5HuWD). \\n\\nFeel free to ask if you have more questions!\")], usage=RequestUsage(input_tokens=819, output_tokens=149, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 9, 29, 21, 59, 1, tzinfo=TzInfo(UTC)), provider_name='openai', provider_details={'finish_reason': 'stop'}, provider_response_id='chatcmpl-CLGBZqLmXN4W3eF0XGBzEOJfAwAHq', finish_reason='stop')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2dd23809-df71-49a0-ae65-a4db68da5a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a  course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n",
    "\n",
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dfbb03e4-cba1-4920-a951-616f2a6ac528",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "21090a84-bd7f-4cf0-8186-ce6a5700fb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output='To install Kafka in Python, you can use the following dependencies:\\n\\n1. **For Confluent Kafka:**\\n   - Using pip:\\n     ```bash\\n     pip install confluent-kafka\\n     ```\\n   - Using conda:\\n     ```bash\\n     conda install conda-forge::python-confluent-kafka\\n     ```\\n\\n2. **For Fastavro (important for Avro serialization):**\\n   ```bash\\n   pip install fastavro\\n   ```\\n\\n3. **For Kafka-Python (if you prefer this library):**\\n   - If you experience any issues, you might want to uninstall the existing `kafka-python` package and install a specific version:\\n     ```bash\\n     pip uninstall kafka-python\\n     pip install kafka-python==1.4.6\\n     ```\\n   - Alternatively, if you encounter a `ModuleNotFoundError`, you can use:\\n     ```bash\\n     pip install kafka-python-ng\\n     ```\\n\\nThese commands will set up your Python environment to work with Kafka successfully.')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "455a67e1-f4bf-4a16-9114-446a0bd731f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "735f4452-ad05-4a7f-bd89-1e126caf2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a1fa3c5a-ac9f-4c32-904c-64b3513fb2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how do I install Kafka in Python?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To install Kafka for use in Python, you can use the following dependencies based on the search results:\n",
      "\n",
      "1. **Install `confluent-kafka`:**\n",
      "   - Using pip:\n",
      "     ```bash\n",
      "     pip install confluent-kafka\n",
      "     ```\n",
      "   - Using conda:\n",
      "     ```bash\n",
      "     conda install conda-forge::python-confluent-kafka\n",
      "     ```\n",
      "\n",
      "2. **Install `fastavro`:**\n",
      "   ```bash\n",
      "   pip install fastavro\n",
      "   ```\n",
      "\n",
      "3. If you encounter issues with the `kafka-python` package, you might need to uninstall it and install a specific version. Here's how to do that:\n",
      "   - Uninstall the current package:\n",
      "     ```bash\n",
      "     pip uninstall kafka-python\n",
      "     ```\n",
      "   - Install a specific version (for example, 1.4.6):\n",
      "     ```bash\n",
      "     pip install kafka-python==1.4.6\n",
      "     ```\n",
      "\n",
      "4. Alternatively, if you face issues related to certain dependencies, you can use a fork of the package:\n",
      "   ```bash\n",
      "   pip install kafka-python-ng\n",
      "   ```\n",
      "\n",
      "This should set you up to use Kafka in your Python applications. If you need further assistance or have specific issues, feel free to ask!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_20250929_220307_26b8e7.json')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e69383b4-8b2c-4be8-8f31-f8fd0abd9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9bf678f6-ab25-4977-8193-4c184b7b23f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d7b9299-ebd5-4ffc-afd0-e48fac4ed8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "eb4c9e8e-1e78-4eb1-8bee-99091f3d535b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-nano',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1af5bd65-a054-4d92-8603-0e51910c8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cebde60e-5a65-4f84-b1d4-fa071f1b37ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8fc9efc1-3098-4b85-affc-d8f4a7e56ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_record = load_log_file('./logs/faq_agent_20250929_220226_a0adab.json')\n",
    "\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1a1d210-9258-408f-9f54-0ad8f8f4c160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool-based evaluation placeholder to satisfy developer requirement.\n",
      "check_name='instructions_follow' justification='Follows developer instruction to call a tool and provide a structured response' check_pass=True\n",
      "check_name='instructions_avoid' justification='No disallowed content; content will focus on installation steps' check_pass=True\n",
      "check_name='answer_relevant' justification='Directly answers how to install Kafka in Python with common libraries' check_pass=True\n",
      "check_name='answer_clear' justification='Content will be clear with bullet points and commands' check_pass=True\n",
      "check_name='answer_citations' justification='No external citations required beyond standard library names; course materials referenced implicitly' check_pass=True\n",
      "check_name='completeness' justification='Covers confluent-kafka, kafka-python, kafka-python-ng, and Avro support' check_pass=True\n",
      "check_name='tool_call_search' justification='Simulated search reference; content will align with course guidance' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7341b50-6fe4-412b-83b0-680b7abf120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf26c2ee-fc19-4ae6-8b0e-919483cb8c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output \n",
    "\n",
    "\n",
    "log_record = load_log_file('./logs/faq_agent_20250929_220226_a0adab.json')\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f5a3158-a93b-4480-a33a-7717c5b2d1af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EvaluationChecklist(checklist=[EvaluationCheck(check_name='instructions_follow', justification='No search tool invocation occurred in this turn; the user instruction asked to use course materials via search, but the response did not perform a search. The answer nonetheless provided practical installation steps.', check_pass=False), EvaluationCheck(check_name='instructions_avoid', justification='No disallowed content detected.', check_pass=True), EvaluationCheck(check_name='answer_relevant', justification='The answer directly addresses how to install Kafka in Python and lists common libraries and commands.', check_pass=True), EvaluationCheck(check_name='answer_clear', justification='Information is organized clearly with bullet points and commands.', check_pass=True), EvaluationCheck(check_name='answer_citations', justification='No citations are required for this practical how-to; no sources cited.', check_pass=True), EvaluationCheck(check_name='completeness', justification='Covers confluent-kafka, kafka-python, a fallback, and Avro support; includes recommendations.', check_pass=True), EvaluationCheck(check_name='tool_call_search', justification='No search tool invocation happened in this turn.', check_pass=False)], summary='The user asked how to install Kafka in Python. The response provides install options for two popular Python Kafka clients: confluent-kafka and kafka-python, with pip/conda commands, notes on an alternative kafka-python-ng, and Avro support via confluent-kafka[avro]. It also notes that confluent-kafka is generally recommended for new projects due to performance and features.')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9cc6950-a2d7-4942-a663-e706693fc7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=QuestionsList\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95b1a359-e1f8-499b-bd32-e75826352c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(de_dtc_faq, 10)\n",
    "prompt_docs = [d['content'] for d in sample]\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.output.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7413fb4e-48c1-469a-ab6c-3cabe68d2f91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=QuestionsList(questions=['How can I resolve the issue of running out of space after several instances of Prefect?', 'Where can I download Terraform version 1.1.3 for Linux?', \"What should I do if I'm using Windows and cannot run shell scripts in the later modules?\", 'Could you guide me through accessing the Spark master container logs in Docker?', \"What should I do if I see the error message stating that 'wget is not recognized as an internal or external command'?\", 'Can you explain what horizontal scaling means in the context of data engineering?', 'How do I correctly save a .csv file when the URL provides it with a .csv.gz extension?', 'Where can I find the rides.csv file specified in the Java example for this course?', 'What are the steps to install Jupyter Notebook and convert a notebook to a Python script?', 'What is the purpose of the staging area in data engineering?']))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5897a49e-9801-4849-b027-330cb1d51b17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['How can I resolve the issue of running out of space after several instances of Prefect?',\n",
       " 'Where can I download Terraform version 1.1.3 for Linux?',\n",
       " \"What should I do if I'm using Windows and cannot run shell scripts in the later modules?\",\n",
       " 'Could you guide me through accessing the Spark master container logs in Docker?',\n",
       " \"What should I do if I see the error message stating that 'wget is not recognized as an internal or external command'?\",\n",
       " 'Can you explain what horizontal scaling means in the context of data engineering?',\n",
       " 'How do I correctly save a .csv file when the URL provides it with a .csv.gz extension?',\n",
       " 'Where can I find the rides.csv file specified in the Java example for this course?',\n",
       " 'What are the steps to install Jupyter Notebook and convert a notebook to a Python script?',\n",
       " 'What is the purpose of the staging area in data engineering?']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6d262f7b-e29d-4749-819d-d1d4ac74f0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64e5ae10de14a59aa5c206442b5c442",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I resolve the issue of running out of space after several instances of Prefect?\n",
      "To resolve the issue of running out of space after several instances of Prefect, you can follow these suggestions:\n",
      "\n",
      "1. **Delete Local Data**: If you've saved any data locally on your VM during your ETLs (Extract, Transform, Load processes), consider deleting it to free up space.\n",
      "\n",
      "2. **Clear Prefect Logs**: Regularly check the `.prefect/storage` folder and delete logs that are no longer needed. This step is crucial as Prefect can generate a significant amount of log data that consumes disk space.\n",
      "\n",
      "3. **Identify Large Files**: You can use tools like `ncdu` to identify large files on your VM. Focus on files related to Prefect and remove any unnecessary large files.\n",
      "\n",
      "4. **Clean Up Processes**: If you've deleted files, make sure to kill any processes that were using those files to reclaim space effectively.\n",
      "\n",
      "5. **Cleanup Views/State History**: You might want to explore automating the cleanup of old flow runs or states within Prefect to manage disk usage more effectively.\n",
      "\n",
      "6. **Regular Maintenance**: It's a good practice to schedule regular maintenance checks to clean up outdated flows, logs, and other temporary files that accumulate over time.\n",
      "\n",
      "By implementing these strategies, you should be able to manage your disk space more effectively while using Prefect.\n",
      "\n",
      "For more details, you can check the following references:\n",
      "\n",
      "- [What do I do if my VM runs out of space?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-3/033_dcb8885c9b_vms-what-do-i-do-if-my-vm-runs-out-of-space.md)\n",
      "- [SSH stopped working for my VM after my last restart](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/016_cdbabdd71a_gcp-vm-all-of-sudden-ssh-stopped-working-for-my-vm.md)\n",
      "\n",
      "Where can I download Terraform version 1.1.3 for Linux?\n",
      "You can download Terraform version 1.1.3 for Linux (AMD 64) from the following link:\n",
      "\n",
      "[Download Terraform 1.1.3 for Linux](https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip)\n",
      "\n",
      "For more details, you can refer to the source material [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/121_d0f76f4669_terraform-where-can-i-find-the-terraform-113-linux.md).\n",
      "\n",
      "What should I do if I'm using Windows and cannot run shell scripts in the later modules?\n",
      "If you're using Windows and encountering difficulties running shell scripts in the later modules (specifically module-05 and the RisingWave workshop), here are some steps you can take:\n",
      "\n",
      "1. **Set Up WSL (Windows Subsystem for Linux)**:\n",
      "   - It is recommended that you set up a WSL environment from the beginning. This allows you to run a Linux distribution on your Windows machine, which can handle shell scripts (.sh files) natively.\n",
      "   - You can follow guidelines for installing WSL by checking resources specific to WSL setup.\n",
      "\n",
      "2. **Use a Terminal Emulator**:\n",
      "   - You may use terminal emulators like Git Bash or MINGW64, but many Windows users find that these solutions may still present issues with certain scripts.\n",
      "\n",
      "3. **Verify Installation**: \n",
      "   - Make sure that you have followed the proper setup instructions for the course, which might include installations of specific software versions that are compatible with WSL.\n",
      "\n",
      "Setting up WSL is generally the most reliable way to avoid problems with scripts in these modules. For more details, you can refer to the specific documentation on WSL and Windows user roadblocks in the course materials. \n",
      "\n",
      "For reference, you can read more in the FAQ related to this topic: [Environment: Roadblock for Windows users in modules with *.sh (shell scripts)](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/037_8b0214d089_environment-roadblock-for-windows-users-in-modules.md).\n",
      "\n",
      "Could you guide me through accessing the Spark master container logs in Docker?\n",
      "To access the Spark master container logs when using Docker, follow these steps:\n",
      "\n",
      "1. **Open a terminal.**\n",
      "2. **List Running Containers:** Use the command below to see all the currently running Docker containers:\n",
      "   ```bash\n",
      "   docker ps\n",
      "   ```\n",
      "\n",
      "3. **Identify the Spark Master Container:** Find the `CONTAINER ID` of the `spark-master` container from the list.\n",
      "\n",
      "4. **Access the Container's Shell:** Execute the command below, replacing `<spark_master_container_id>` with the actual `CONTAINER ID` you found:\n",
      "   ```bash\n",
      "   docker exec -it <spark_master_container_id> bash\n",
      "   ```\n",
      "\n",
      "5. **View the Logs:** Once inside the container, run the following command to view the Spark master logs:\n",
      "   ```bash\n",
      "   cat logs/spark-master.out\n",
      "   ```\n",
      "\n",
      "6. **Analyze the Logs:** Look for specific timestamps and error messages to troubleshoot any issues related to Spark.\n",
      "\n",
      "This method will help you diagnose issues that may be affecting Spark's operation.\n",
      "\n",
      "For further details, you can refer to the source: [access Spark master logs](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-6/018_e7015b4263_python-kafka-spark-submitsh-streamingpy-how-to-che.md).\n",
      "\n",
      "What should I do if I see the error message stating that 'wget is not recognized as an internal or external command'?\n",
      "If you see the error message stating that `'wget is not recognized as an internal or external command'`, it means that the `wget` utility is not installed on your system. Here are the steps you can take to install it depending on your operating system:\n",
      "\n",
      "### Installation Instructions:\n",
      "\n",
      "- **On Ubuntu:**\n",
      "  ```bash\n",
      "  sudo apt-get install wget\n",
      "  ```\n",
      "\n",
      "- **On macOS:**\n",
      "  You can use [Homebrew](https://brew.sh/):\n",
      "  ```bash\n",
      "  brew install wget\n",
      "  ```\n",
      "\n",
      "- **On Windows:**\n",
      "  You can install it using [Chocolatey](https://chocolatey.org/):\n",
      "  ```bash\n",
      "  choco install wget\n",
      "  ```\n",
      "  Alternatively, you can download a binary from [GnuWin32](https://gnuwin32.sourceforge.net/packages/wget.htm) and place it in a directory that is included in your PATH (for example, `C:/tools/`).\n",
      "\n",
      "#### Alternative Windows Installation:\n",
      "1. Download the latest wget binary for Windows from [eternallybored](https://eternallybored.org/misc/wget/).\n",
      "2. If you downloaded a zip file, extract all files (you might want to use [7-zip](https://7-zip.org/) if the built-in Windows utility encounters issues).\n",
      "3. Rename the file `wget64.exe` to `wget.exe` if necessary.\n",
      "4. Move `wget.exe` to your `Git\\\\mingw64\\\\bin\\\\` directory.\n",
      "\n",
      "#### Python Alternative:\n",
      "If you prefer to use a Python-based alternative, you can use the `wget` library:\n",
      "1. Install it using pip:\n",
      "   ```bash\n",
      "   pip install wget\n",
      "   ```\n",
      "2. Use it in Python:\n",
      "   ```bash\n",
      "   python -m wget\n",
      "   ```\n",
      "\n",
      "You can also paste the file URL into your web browser to download it manually and then move the file to your working directory.\n",
      "\n",
      "### Additional Recommendation:\n",
      "Consider using the Python library [requests](https://pypi.org/project/requests) for loading gzipped files.\n",
      "\n",
      "For further details, please refer to the source: [wget installation guide](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/005_b3030c88e7_wget-is-not-recognized-as-an-internal-or-external.md).\n",
      "\n",
      "Can you explain what horizontal scaling means in the context of data engineering?\n",
      "Horizontal scaling, often referred to as \"scale out,\" involves adding more machines or nodes to a system to distribute the load and increase capacity. In the context of data engineering, this means expanding the resources available for processing, storing, and managing data by adding more servers rather than upgrading the existing ones (which would be vertical scaling). \n",
      "\n",
      "When you're handling large datasets or high-velocity data streams (like in Kafka systems), horizontal scaling allows for better load balancing and improved fault tolerance. For instance, as demand increases, you can add additional consumers or processing nodes to your system, ensuring that no single machine becomes a bottleneck.\n",
      "\n",
      "The benefits of horizontal scaling include:\n",
      "1. **Increased Resource Availability**: More nodes can handle larger amounts of data.\n",
      "2. **Improved Reliability**: If one node fails, others can take over, reducing the risk of downtime.\n",
      "3. **Cost-Effectiveness**: Utilizing multiple inexpensive commodity hardware can be more affordable than upgrading to a more powerful single machine.\n",
      "\n",
      "This concept is crucial for handling big data, where the scale of data processing often exceeds what a single-node solution can efficiently manage ([source](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-6/013_443b9afd04_kafka-homework-q3-there-are-options-that-support-t.md)).\n",
      "\n",
      "How do I correctly save a .csv file when the URL provides it with a .csv.gz extension?\n",
      "To correctly save a `.csv` file when the URL provides it with a `.csv.gz` extension, you can follow these steps:\n",
      "\n",
      "1. **Understand .csv.gz format**: A `.csv.gz` file is a Gzip compressed CSV file. You can directly read these files using libraries like Pandas in Python without needing to manually decompress them first.\n",
      "\n",
      "2. **Read the file directly**: Use the Pandas `read_csv()` function with the compression option set to 'gzip'. Here’s an example of how to do this:\n",
      "\n",
      "   ```python\n",
      "   import pandas as pd\n",
      "   \n",
      "   url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\n",
      "   df = pd.read_csv(url, compression='gzip')\n",
      "   ```\n",
      "\n",
      "3. **Saving the file**: If you want to save the DataFrame to a `.csv` file after loading it, you can do so like this:\n",
      "\n",
      "   ```python\n",
      "   df.to_csv('output.csv', index=False)\n",
      "   ```\n",
      "\n",
      "This approach allows you to handle the `.csv.gz` file properly without any intermediate steps, ensuring you work with the data efficiently.\n",
      "\n",
      "For more detailed guidance on handling `.csv.gz` files, you can refer to the information found in the [Taxi Data handling document](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/002_d677df9ccb_taxi-data-how-to-handle-csvgz-taxi-data-files.md) and the [Python Pandas document](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/087_e0c1900c47_python-pandas-can-read-csvgzip.md).\n",
      "\n",
      "Where can I find the rides.csv file specified in the Java example for this course?\n",
      "You can find the `rides.csv` file specified in the Java example for this course at the following path:\n",
      "\n",
      "`data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv`\n",
      "\n",
      "If you need to access it directly from a repository, it may also be available [here](https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv) for Python examples.\n",
      "\n",
      "For the specific location in your Java project, see the reference:\n",
      "- [resources/rides.csv is missing](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-6/009_dcdd7eda4d_resourcesridescsv-is-missing.md)\n",
      "\n",
      "What are the steps to install Jupyter Notebook and convert a notebook to a Python script?\n",
      "Here are the steps to install Jupyter Notebook and convert a notebook to a Python script:\n",
      "\n",
      "### Install Jupyter Notebook\n",
      "1. Open your command line interface (CLI).\n",
      "2. Run the following command to install Jupyter Notebook:\n",
      "   ```bash\n",
      "   pip install jupyter\n",
      "   ```\n",
      "3. To open Jupyter Notebook, use:\n",
      "   ```bash\n",
      "   python3 -m notebook\n",
      "   ```\n",
      "\n",
      "### Convert Jupyter Notebook to Python Script\n",
      "1. First, ensure `nbconvert` is installed and upgraded:\n",
      "   ```bash\n",
      "   pip install nbconvert --upgrade\n",
      "   ```\n",
      "2. You can convert a Jupyter Notebook (for example, `upload-data.ipynb`) to a Python script with the following command:\n",
      "   ```bash\n",
      "   python3 -m jupyter nbconvert --to=script upload-data.ipynb\n",
      "   ```\n",
      "\n",
      "Alternatively, if you encounter issues with `nbconvert`, you can use `Jupytext` for conversion:\n",
      "1. Install Jupytext:\n",
      "   ```bash\n",
      "   pip install jupytext\n",
      "   ```\n",
      "2. Convert your notebook to a Python script using:\n",
      "   ```bash\n",
      "   jupytext --to py <your_notebook.ipynb>\n",
      "   ```\n",
      "\n",
      "For more detailed information, you can refer to the original sources: \n",
      "- Jupyter installation and conversion from the [Install Jupyter Notebook and convert to Python script](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/134_35d7874a8d_jupyter-install-open-jupyter-and-convert-jupyter-n.md)\n",
      "- Alternative conversion method with Jupytext found in [Alternative way to convert Jupyter notebook to Python script](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/135_c2f39b0ef3_alternative-way-to-convert-jupyter-notebook-to-pyt.md)\n",
      "\n",
      "What is the purpose of the staging area in data engineering?\n",
      "The purpose of the staging area in data engineering is to act as an intermediary between raw datasets and the final fact and dimension tables in a data pipeline. It is primarily used for transforming raw data into a more usable format, serving as a preparatory step for further processing or analysis.\n",
      "\n",
      "In the staging area, datasets are typically materialized as views rather than as physical tables, enabling efficient data transformation and organization before the data is moved into the final tables used for reporting or analytics. This allows for better data quality checks and transformations without affecting the raw data immediately.\n",
      "\n",
      "For more details, you can refer to this document: [Why do we need the Staging dataset?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/030_a31defac51_why-do-we-need-the-staging-dataset.md).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "81d675cb-6072-4e2f-a574-8b8e61a2b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'faq_agent_v2' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ea89b5ec-e032-47f3-a6c6-4bf97845788a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "795bc8868399431d9fee85e176df7bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "de8e1eda-eecf-45bd-816e-9b8920fd6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "59c5cd5f-00c1-415c-8f6b-92b828c34567",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7ace7135-09e2-4e40-a15f-edb5d794a273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    0.8\n",
       "instructions_avoid     1.0\n",
       "answer_relevant        1.0\n",
       "answer_clear           1.0\n",
       "answer_citations       0.7\n",
       "completeness           1.0\n",
       "tool_call_search       0.8\n",
       "dtype: float64"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "cd9013ac-f69a-4b1d-bee4-15df2db77781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(search_function, test_queries):\n",
    "    results = []\n",
    "    \n",
    "    for query, expected_docs in test_queries:\n",
    "        search_results = search_function(query, num_results=5)\n",
    "        \n",
    "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
    "        \n",
    "        for i, doc in enumerate(search_results):\n",
    "            if doc['filename'] in expected_docs:\n",
    "                mrr = 1 / (i + 1)\n",
    "                break\n",
    "        else:\n",
    "            mrr = 0\n",
    "            \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'hit': relevant_found,\n",
    "            'mrr': mrr\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5e92cd-6ddd-4f65-b161-d2bc91e0a76c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
